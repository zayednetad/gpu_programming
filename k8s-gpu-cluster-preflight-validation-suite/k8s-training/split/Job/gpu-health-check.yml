---
apiVersion: batch/v1
kind: Job
metadata:
  name: gpu-health-check
  namespace: gpu-preflight
  labels:
    app: gpu-health-check
    component: gpu-health
spec:
  # Set parallelism/completions to your GPU node count for full-cluster coverage.
  parallelism: 2
  completions: 2
  backoffLimit: 2
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: gpu-health-check
    spec:
      restartPolicy: Never
      serviceAccountName: preflight-sa
      nodeSelector:
        nebius.com/gpu: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
        - key: gpu-preflight
          operator: Exists
          effect: NoSchedule
      containers:
        - name: gpu-health
          image: nvcr.io/nvidia/cuda:12.4.0-devel-ubuntu22.04
          command: ["/bin/bash", "-c"]
          args:
            - "#!/bin/bash\nset -e\n\n# Install dependencies\napt-get update && apt-get install -y jq curl bc > /dev/null 2>&1\ncurl -LO \"https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl\" > /dev/null 2>&1\nchmod +x kubectl && mv kubectl /usr/local/bin/\n\nRESULTS_DIR=\"/results\"\nSHARED_DIR=\"/shared-results\"\nNODE_NAME=\"${NODE_NAME:-unknown}\"\nCONFIG_FILE=\"/config/config.json\"\nmkdir -p \"${RESULTS_DIR}\" \"${SHARED_DIR}\"\n\n# Load thresholds\nEXPECTED_GPUS=$(jq -r '.gpu.expected_count_per_node' \"${CONFIG_FILE}\")\nMAX_TEMP=$(jq -r '.gpu.max_temperature_celsius' \"${CONFIG_FILE}\")\nMAX_ECC=$(jq -r '.gpu.max_ecc_errors' \"${CONFIG_FILE}\")\nMAX_UTIL_IDLE=$(jq -r '.gpu.max_utilization_idle' \"${CONFIG_FILE}\")\nMIN_MEMORY_GB=$(jq -r '.gpu.min_memory_gb' \"${CONFIG_FILE}\")\nMIN_TFLOPS=$(jq -r '.gpu.min_matmul_tflops' \"${CONFIG_FILE}\")\nMIN_DRIVER=$(jq -r '.versions.nvidia_driver_min' \"${CONFIG_FILE}\")\nMIN_CUDA=$(jq -r '.versions.cuda_version_min' \"${CONFIG_FILE}\")\nMIN_NCCL=$(jq -r '.versions.nccl_version_min' \"${CONFIG_FILE}\")\n\nrun_health_check() {\n  TIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n  RESULT_FILE=\"${RESULTS_DIR}/${NODE_NAME}-gpu-health.json\"\n  \n  echo \"==========================================\"\n  echo \"GPU Health Check: ${NODE_NAME}\"\n  echo \"Timestamp: ${TIMESTAMP}\"\n  echo \"==========================================\"\n  \n  OVERALL_STATUS=\"PASS\"\n  ISSUES=\"\"\n  \n  # --------------------------------------------------------\n  # 3.1 Driver / CUDA / NCCL Version Checks\n  # --------------------------------------------------------\n  echo \"\"\n  echo \"=== Version Checks ===\"\n  \n  # NVIDIA Driver\n  DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1 | tr -d ' ')\n  if [ -z \"$DRIVER_VERSION\" ]; then\n    DRIVER_STATUS=\"FAIL\"; OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}driver_not_found;\"\n  elif [ \"$(printf '%s\\n' \"$MIN_DRIVER\" \"$DRIVER_VERSION\" | sort -V | head -n1)\" = \"$MIN_DRIVER\" ]; then\n    DRIVER_STATUS=\"PASS\"\n  else\n    DRIVER_STATUS=\"FAIL\"; OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}driver_version_low(${DRIVER_VERSION}<${MIN_DRIVER});\"\n  fi\n  echo \"  NVIDIA Driver: ${DRIVER_VERSION} (min: ${MIN_DRIVER}) [${DRIVER_STATUS}]\"\n  \n  # CUDA Version (prefer driver-reported runtime support)\n  CUDA_VERSION=$(nvidia-smi 2>/dev/null | sed -n 's/.*CUDA Version: \\([0-9.]*\\).*/\\1/p' | head -1)\n  if [ -z \"${CUDA_VERSION}\" ]; then\n    CUDA_VERSION=$(nvcc --version 2>/dev/null | grep \"release\" | sed -n 's/.*release \\([0-9]*\\.[0-9]*\\).*/\\1/p' || echo \"not_found\")\n  fi\n  if [ \"$CUDA_VERSION\" = \"not_found\" ]; then\n    CUDA_STATUS=\"FAIL\"; OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}cuda_not_found;\"\n  elif [ \"$(printf '%s\\n' \"$MIN_CUDA\" \"$CUDA_VERSION\" | sort -V | head -n1)\" = \"$MIN_CUDA\" ]; then\n    CUDA_STATUS=\"PASS\"\n  else\n    CUDA_STATUS=\"FAIL\"; OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}cuda_version_low(${CUDA_VERSION}<${MIN_CUDA});\"\n  fi\n  echo \"  CUDA Toolkit: ${CUDA_VERSION} (min: ${MIN_CUDA}) [${CUDA_STATUS}]\"\n  \n  # NCCL Version\n  NCCL_VERSION=$(python3 -c \"import torch; print(torch.cuda.nccl.version())\" 2>/dev/null || \\\n                 ldconfig -p 2>/dev/null | grep -i nccl | head -1 | grep -oP '\\d+\\.\\d+' || echo \"not_found\")\n  if [ \"$NCCL_VERSION\" = \"not_found\" ] || [ -z \"$NCCL_VERSION\" ]; then\n    NCCL_STATUS=\"WARN\"\n    NCCL_VERSION=\"system_default\"\n  else\n    if [ \"$(printf '%s\\n' \"$MIN_NCCL\" \"$NCCL_VERSION\" | sort -V | head -n1)\" = \"$MIN_NCCL\" ]; then\n      NCCL_STATUS=\"PASS\"\n    else\n      NCCL_STATUS=\"WARN\"; ISSUES=\"${ISSUES}nccl_version_low;\"\n    fi\n  fi\n  echo \"  NCCL: ${NCCL_VERSION} (min: ${MIN_NCCL}) [${NCCL_STATUS}]\"\n  \n  # --------------------------------------------------------\n  # 3.2 GPU Visibility & Hardware Health\n  # --------------------------------------------------------\n  echo \"\"\n  echo \"=== GPU Hardware Health ===\"\n  \n  GPU_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | wc -l)\n  if [ \"$GPU_COUNT\" -ge \"$EXPECTED_GPUS\" ]; then\n    GPU_COUNT_STATUS=\"PASS\"\n  else\n    GPU_COUNT_STATUS=\"FAIL\"; OVERALL_STATUS=\"FAIL\"\n    ISSUES=\"${ISSUES}gpu_count(found:${GPU_COUNT},expected:${EXPECTED_GPUS});\"\n  fi\n  echo \"  GPU Count: ${GPU_COUNT}/${EXPECTED_GPUS} [${GPU_COUNT_STATUS}]\"\n  \n  GPU_HEALTH_JSON=\"[\"\n  for i in $(seq 0 $((GPU_COUNT - 1))); do\n    GPU_INFO=$(nvidia-smi -i $i --query-gpu=index,name,temperature.gpu,memory.total,ecc.mode.current,ecc.errors.corrected.volatile.total,utilization.gpu,power.draw --format=csv,noheader,nounits 2>/dev/null)\n    IFS=',' read -r idx name temp mem_total ecc_mode ecc_errors util power <<< \"$GPU_INFO\"\n    \n    idx=$(echo \"$idx\" | xargs); name=$(echo \"$name\" | xargs)\n    temp=$(echo \"$temp\" | xargs); ecc_mode=$(echo \"$ecc_mode\" | xargs)\n    ecc_errors=$(echo \"$ecc_errors\" | xargs); util=$(echo \"$util\" | xargs)\n    mem_gb=$((${mem_total:-0} / 1024))\n    \n    gpu_status=\"PASS\"; gpu_issues=\"\"\n    \n    # ECC Status\n    if [ \"${ecc_mode}\" != \"Enabled\" ]; then\n      gpu_issues=\"${gpu_issues}ecc_disabled;\"\n    fi\n    \n    # ECC Errors\n    if [ \"${ecc_errors:-0}\" != \"N/A\" ] && [ \"${ecc_errors:-0}\" != \"0\" ] 2>/dev/null; then\n      if [ \"${ecc_errors}\" -gt \"${MAX_ECC}\" ] 2>/dev/null; then\n        gpu_status=\"FAIL\"; gpu_issues=\"${gpu_issues}ecc_errors(${ecc_errors});\"\n        OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}GPU${idx}:ecc;\"\n      fi\n    fi\n    \n    # Temperature\n    if [ \"${temp:-0}\" -gt \"${MAX_TEMP}\" ] 2>/dev/null; then\n      gpu_status=\"FAIL\"; gpu_issues=\"${gpu_issues}high_temp(${temp}C);\"\n      OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}GPU${idx}:temp;\"\n    fi\n\n    # Memory capacity\n    if [ \"${mem_gb:-0}\" -lt \"${MIN_MEMORY_GB}\" ] 2>/dev/null; then\n      gpu_status=\"FAIL\"; gpu_issues=\"${gpu_issues}low_memory(${mem_gb}GB<${MIN_MEMORY_GB}GB);\"\n      OVERALL_STATUS=\"FAIL\"; ISSUES=\"${ISSUES}GPU${idx}:memory;\"\n    fi\n    \n    # Utilization (check if stuck at high utilization when idle)\n    if [ \"${util:-0}\" -gt \"${MAX_UTIL_IDLE}\" ] 2>/dev/null; then\n      gpu_issues=\"${gpu_issues}high_idle_util(${util}%);\"\n    fi\n    \n    echo \"  GPU ${idx} (${name}): Temp=${temp}C, ECC=${ecc_mode}, Errors=${ecc_errors:-0}, Util=${util}% [${gpu_status}]\"\n    \n    [ $i -gt 0 ] && GPU_HEALTH_JSON=\"${GPU_HEALTH_JSON},\"\n    GPU_HEALTH_JSON=\"${GPU_HEALTH_JSON}{\\\"gpu_id\\\":${idx},\\\"name\\\":\\\"${name}\\\",\\\"temp_c\\\":${temp:-0},\\\"memory_gb\\\":${mem_gb},\\\"ecc_mode\\\":\\\"${ecc_mode}\\\",\\\"ecc_errors\\\":\\\"${ecc_errors:-0}\\\",\\\"utilization_pct\\\":${util:-0},\\\"status\\\":\\\"${gpu_status}\\\",\\\"issues\\\":\\\"${gpu_issues}\\\"}\"\n  done\n  GPU_HEALTH_JSON=\"${GPU_HEALTH_JSON}]\"\n  \n  # --------------------------------------------------------\n  # 3.2.1 DCGM Diagnostics (per Nebius best practices)\n  # Source: nebius.com/blog/posts/how-we-build-reliable-clusters\n  # Levels: r1=quick, r2=medium (recommended), r3=extended stress\n  # --------------------------------------------------------\n  echo \"\"\n  echo \"=== DCGM Diagnostics (Level 2) ===\"\n  \n  DCGM_STATUS=\"PASS\"\n  DCGM_OUTPUT=\"\"\n  \n  # Check if dcgmi is available\n  if command -v dcgmi &> /dev/null; then\n    echo \"Running dcgmi diag -r 2 (quick validation)...\"\n    DCGM_OUTPUT=$(dcgmi diag -r 2 2>&1) || true\n    \n    # Check for failures in DCGM output\n    if echo \"$DCGM_OUTPUT\" | grep -q \"Fail\"; then\n      DCGM_STATUS=\"FAIL\"\n      OVERALL_STATUS=\"FAIL\"\n      ISSUES=\"${ISSUES}dcgm_diag_failed;\"\n      echo \"  DCGM Diagnostics: FAIL\"\n      echo \"$DCGM_OUTPUT\" | grep -E \"(Fail|Warning)\" | head -10\n    else\n      echo \"  DCGM Diagnostics: PASS\"\n    fi\n  else\n    echo \"  dcgmi not available, skipping DCGM diagnostics\"\n    DCGM_STATUS=\"SKIPPED\"\n  fi\n  \n  # --------------------------------------------------------\n  # 3.3 GPU Functional Test - Matrix Multiply on EACH GPU\n  # --------------------------------------------------------\n  echo \"\"\n  echo \"=== GPU Compute Tests (Matrix Multiply) ===\"\n  \n  # Create CUDA matrix multiply test\n  cat > /tmp/matmul_test.cu << 'CUDA_CODE'\n#include <stdio.h>\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n#include <sys/time.h>\n\n#define N 4096\n#define ITERATIONS 100\n\nint main(int argc, char *argv[]) {\n    int device = 0;\n    if (argc > 1) device = atoi(argv[1]);\n    \n    cudaSetDevice(device);\n    \n    float *d_A, *d_B, *d_C;\n    size_t size = N * N * sizeof(float);\n    \n    cudaMalloc(&d_A, size);\n    cudaMalloc(&d_B, size);\n    cudaMalloc(&d_C, size);\n    \n    // Initialize with random data\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < N*N; i++) h_data[i] = (float)rand() / RAND_MAX;\n    cudaMemcpy(d_A, h_data, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_data, size, cudaMemcpyHostToDevice);\n    free(h_data);\n    \n    cublasHandle_t handle;\n    cublasCreate(&handle);\n    \n    float alpha = 1.0f, beta = 0.0f;\n    \n    // Warmup\n    for (int i = 0; i < 10; i++) {\n        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);\n    }\n    cudaDeviceSynchronize();\n    \n    // Benchmark\n    struct timeval start, end;\n    gettimeofday(&start, NULL);\n    \n    for (int i = 0; i < ITERATIONS; i++) {\n        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, &alpha, d_A, N, d_B, N, &beta, d_C, N);\n    }\n    cudaDeviceSynchronize();\n    \n    gettimeofday(&end, NULL);\n    \n    double elapsed = (end.tv_sec - start.tv_sec) + (end.tv_usec - start.tv_usec) / 1e6;\n    double ops = 2.0 * N * N * N * ITERATIONS;\n    double tflops = ops / elapsed / 1e12;\n    \n    cudaError_t err = cudaGetLastError();\n    \n    printf(\"{\\\"gpu\\\":%d,\\\"tflops\\\":%.2f,\\\"status\\\":\\\"%s\\\"}\\n\", \n           device, tflops, err == cudaSuccess ? \"PASS\" : \"FAIL\");\n    \n    cublasDestroy(handle);\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n    \n    return err == cudaSuccess ? 0 : 1;\n}\nCUDA_CODE\n  \n  COMPUTE_RESULTS=\"[\"\n  COMPUTE_STATUS=\"PASS\"\n  \n  if nvcc /tmp/matmul_test.cu -o /tmp/matmul_test -lcublas 2>/dev/null; then\n    for i in $(seq 0 $((GPU_COUNT - 1))); do\n      RESULT=$(/tmp/matmul_test $i 2>/dev/null || echo \"{\\\"gpu\\\":$i,\\\"tflops\\\":0,\\\"status\\\":\\\"FAIL\\\"}\")\n      TFLOPS=$(echo \"$RESULT\" | grep -oP '\"tflops\":\\K[0-9.]+' || echo \"0\")\n      STATUS=$(echo \"$RESULT\" | grep -oP '\"status\":\"\\K[^\"]+' || echo \"FAIL\")\n      \n      echo \"  GPU ${i}: ${TFLOPS} TFLOPS [${STATUS}]\"\n      \n      if [ \"${STATUS}\" = \"FAIL\" ]; then\n        COMPUTE_STATUS=\"FAIL\"\n        OVERALL_STATUS=\"FAIL\"\n        ISSUES=\"${ISSUES}GPU${i}:compute_failed;\"\n      elif (( $(echo \"${TFLOPS} < ${MIN_TFLOPS}\" | bc -l 2>/dev/null || echo 0) )); then\n        if [ \"${TFLOPS}\" != \"0\" ]; then\n          COMPUTE_STATUS=\"WARN\"\n          ISSUES=\"${ISSUES}GPU${i}:low_tflops(${TFLOPS});\"\n        fi\n      fi\n      \n      [ $i -gt 0 ] && COMPUTE_RESULTS=\"${COMPUTE_RESULTS},\"\n      COMPUTE_RESULTS=\"${COMPUTE_RESULTS}{\\\"gpu_id\\\":${i},\\\"tflops\\\":${TFLOPS},\\\"status\\\":\\\"${STATUS}\\\"}\"\n    done\n  else\n    echo \"  Warning: Could not compile CUDA test, using simple validation\"\n    COMPUTE_STATUS=\"WARN\"\n    for i in $(seq 0 $((GPU_COUNT - 1))); do\n      [ $i -gt 0 ] && COMPUTE_RESULTS=\"${COMPUTE_RESULTS},\"\n      COMPUTE_RESULTS=\"${COMPUTE_RESULTS}{\\\"gpu_id\\\":${i},\\\"tflops\\\":0,\\\"status\\\":\\\"SKIPPED\\\"}\"\n    done\n  fi\n  COMPUTE_RESULTS=\"${COMPUTE_RESULTS}]\"\n  \n  # --------------------------------------------------------\n  # Generate JSON Report\n  # --------------------------------------------------------\n  cat > \"${RESULT_FILE}\" << EOF\n{\n  \"test_type\": \"gpu_health\",\n  \"node\": \"${NODE_NAME}\",\n  \"timestamp\": \"${TIMESTAMP}\",\n  \"overall_status\": \"${OVERALL_STATUS}\",\n  \"versions\": {\n    \"driver\": {\"version\": \"${DRIVER_VERSION}\", \"min\": \"${MIN_DRIVER}\", \"status\": \"${DRIVER_STATUS}\"},\n    \"cuda\": {\"version\": \"${CUDA_VERSION}\", \"min\": \"${MIN_CUDA}\", \"status\": \"${CUDA_STATUS}\"},\n    \"nccl\": {\"version\": \"${NCCL_VERSION}\", \"min\": \"${MIN_NCCL}\", \"status\": \"${NCCL_STATUS}\"}\n  },\n  \"gpu_count\": {\"found\": ${GPU_COUNT}, \"expected\": ${EXPECTED_GPUS}, \"status\": \"${GPU_COUNT_STATUS}\"},\n  \"gpu_health\": ${GPU_HEALTH_JSON},\n  \"dcgm_diagnostics\": {\"level\": 2, \"status\": \"${DCGM_STATUS}\"},\n  \"compute_test\": {\"status\": \"${COMPUTE_STATUS}\", \"min_tflops\": ${MIN_TFLOPS}, \"results\": ${COMPUTE_RESULTS}},\n  \"issues\": \"${ISSUES}\"\n}\nEOF\n  \n  # Copy to shared storage\n  cp \"${RESULT_FILE}\" \"${SHARED_DIR}/\" 2>/dev/null || true\n  \n  echo \"\"\n  echo \"Overall Status: ${OVERALL_STATUS}\"\n  [ -n \"${ISSUES}\" ] && echo \"Issues: ${ISSUES}\"\n  \n  # --------------------------------------------------------\n  # 3.4 Node Isolation - Label/Taint failed nodes\n  # --------------------------------------------------------\n  if [ \"${OVERALL_STATUS}\" = \"FAIL\" ]; then\n    echo \"Marking node as FAILED...\"\n    kubectl label nodes \"${NODE_NAME}\" preflight-status=failed --overwrite 2>/dev/null || true\n    kubectl taint nodes \"${NODE_NAME}\" gpu-preflight=failed:NoSchedule --overwrite 2>/dev/null || true\n  else\n    echo \"Marking node as PASSED...\"\n    kubectl label nodes \"${NODE_NAME}\" preflight-status=passed --overwrite 2>/dev/null || true\n    kubectl taint nodes \"${NODE_NAME}\" gpu-preflight=failed:NoSchedule- 2>/dev/null || true\n  fi\n}\n\n# One-shot run\nrun_health_check\n[ \"${OVERALL_STATUS}\" = \"FAIL\" ] && exit 1 || exit 0\n"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          resources:
            limits:
              nvidia.com/gpu: "8"
            requests:
              cpu: "1"
              memory: "4Gi"
          volumeMounts:
            - name: results
              mountPath: /results
            - name: shared-results
              mountPath: /shared-results
            - name: config
              mountPath: /config
          securityContext:
            privileged: true
      volumes:
        - name: results
          hostPath:
            path: /var/log/gpu-preflight
            type: DirectoryOrCreate
        - name: shared-results
          hostPath:
            path: /mnt/data
            type: Directory
        - name: config
          configMap:
            name: preflight-config
# =============================================================================
# SECTION 4: INTRA-NODE GPU FABRIC (NVLink / NVSwitch)
# =============================================================================
# ☑ NCCL intra-node collective test
# ☑ Restrict to GPUs on same node
# ☑ Measure bandwidth
# ☑ Measure latency
# ☑ Compare against thresholds
# ☑ Flag degraded communication
# ☑ Mark node failed if below threshold
# =============================================================================
