---
apiVersion: batch/v1
kind: Job
metadata:
  name: preflight-aggregator
  namespace: gpu-preflight
  labels:
    app: preflight-aggregator
    component: aggregator
spec:
  backoffLimit: 3
  ttlSecondsAfterFinished: 86400
  template:
    metadata:
      labels:
        app: preflight-aggregator
    spec:
      restartPolicy: Never
      serviceAccountName: preflight-sa
      containers:
        - name: aggregator
          image: ubuntu:22.04
          command: ["/bin/bash", "-c"]
          args:
            - "#!/bin/bash\nset -e\n\n# Install dependencies\napt-get update && apt-get install -y curl jq bc grep > /dev/null 2>&1\ncurl -LO \"https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl\" > /dev/null 2>&1\nchmod +x kubectl && mv kubectl /usr/local/bin/\n\nRESULTS_DIR=\"/results\"\nSHARED_DIR=\"/shared-results\"\nTIMESTAMP=$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\nFINAL_REPORT=\"${SHARED_DIR}/cluster-preflight-report.json\"\n\necho \"==========================================\"\necho \"PREFLIGHT RESULTS AGGREGATOR\"\necho \"Timestamp: ${TIMESTAMP}\"\necho \"==========================================\"\n\nwait_for_job_completion() {\n  local job_name=\"$1\"\n  local timeout_s=\"${2:-1800}\"\n  local interval_s=10\n  local elapsed_s=0\n\n  echo \"Waiting for Job/${job_name} to complete...\"\n  while [ \"${elapsed_s}\" -lt \"${timeout_s}\" ]; do\n    completions=$(kubectl get job \"${job_name}\" -n gpu-preflight -o jsonpath='{.spec.completions}' 2>/dev/null || echo \"\")\n    succeeded=$(kubectl get job \"${job_name}\" -n gpu-preflight -o jsonpath='{.status.succeeded}' 2>/dev/null || echo \"0\")\n    failed=$(kubectl get job \"${job_name}\" -n gpu-preflight -o jsonpath='{.status.failed}' 2>/dev/null || echo \"0\")\n\n    [ -z \"${completions}\" ] && completions=1\n    [ -z \"${succeeded}\" ] && succeeded=0\n    [ -z \"${failed}\" ] && failed=0\n\n    if [ \"${succeeded}\" -ge \"${completions}\" ] 2>/dev/null; then\n      echo \"  Job/${job_name}: complete (${succeeded}/${completions})\"\n      return 0\n    fi\n\n    if [ \"${failed}\" -gt \"0\" ] 2>/dev/null; then\n      echo \"  Job/${job_name}: failed (failed=${failed})\"\n      return 1\n    fi\n\n    sleep \"${interval_s}\"\n    elapsed_s=$((elapsed_s + interval_s))\n  done\n\n  echo \"  Job/${job_name}: timed out after ${timeout_s}s\"\n  return 1\n}\n\nWAIT_FAILURES=\"\"\nfor job in gpu-health-check nccl-intra-node-test network-tcp-test storage-benchmark scheduler-validation; do\n  wait_for_job_completion \"${job}\" 1800 || WAIT_FAILURES=\"${WAIT_FAILURES}${job},\"\ndone\n\n# Get GPU nodes\nGPU_NODES=$(kubectl get nodes -l nebius.com/gpu=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo \"\")\nTOTAL_GPU_NODES=$(echo $GPU_NODES | wc -w)\n\necho \"Found ${TOTAL_GPU_NODES} GPU nodes: ${GPU_NODES}\"\n\n# Initialize counters\nCLUSTER_STATUS=\"READY\"\nHEALTHY_NODES=0\nFAILED_NODES=\"\"\nDEGRADED_COMPONENTS=\"\"\n[ -n \"${WAIT_FAILURES}\" ] && DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}job_wait:${WAIT_FAILURES}\"\n\n# Category status\nGPU_CATEGORY=\"PASS\"\nNCCL_CATEGORY=\"PASS\"\nNETWORK_CATEGORY=\"PASS\"\nSTORAGE_CATEGORY=\"PASS\"\nSCHEDULER_CATEGORY=\"PASS\"\n\n# Result coverage counters\nGPU_RESULTS_FOUND=0\nNCCL_RESULTS_FOUND=0\nNETWORK_RESULTS_FOUND=0\nSTORAGE_RESULTS_FOUND=0\n\nif [ \"${TOTAL_GPU_NODES}\" -eq 0 ]; then\n  CLUSTER_STATUS=\"NOT_READY\"\n  GPU_CATEGORY=\"FAIL\"\n  DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}gpu_nodes:none,\"\nfi\n\nif [ -n \"${WAIT_FAILURES}\" ]; then\n  CLUSTER_STATUS=\"NOT_READY\"\nfi\n\n# Collect results arrays\nGPU_RESULTS=\"[\"\nNCCL_RESULTS=\"[\"\nNETWORK_RESULTS=\"[\"\nSTORAGE_RESULTS=\"[\"\n\nFIRST_GPU=true; FIRST_NCCL=true; FIRST_NET=true; FIRST_STOR=true\n\nfor node in ${GPU_NODES}; do\n  echo \"\"\n  echo \"=== Processing ${node} ===\"\n  NODE_STATUS=\"PASS\"\n  \n  # GPU Health\n  GPU_FOUND=false\n  for dir in \"${RESULTS_DIR}\" \"${SHARED_DIR}\"; do\n    if [ -f \"${dir}/${node}-gpu-health.json\" ]; then\n      GPU_FOUND=true\n      GPU_RESULTS_FOUND=$((GPU_RESULTS_FOUND + 1))\n      STATUS=$(grep -o '\"overall_status\"[[:space:]]*:[[:space:]]*\"[^\"]*\"' \"${dir}/${node}-gpu-health.json\" | head -1 | cut -d'\"' -f4)\n      echo \"  GPU Health: ${STATUS}\"\n      if [ \"${STATUS}\" = \"FAIL\" ]; then\n        NODE_STATUS=\"FAIL\"; GPU_CATEGORY=\"FAIL\"\n        CLUSTER_STATUS=\"NOT_READY\"; FAILED_NODES=\"${FAILED_NODES}${node},\"\n        kubectl taint nodes \"${node}\" gpu-preflight=failed:NoSchedule --overwrite 2>/dev/null || true\n        kubectl label nodes \"${node}\" preflight-status=failed --overwrite 2>/dev/null || true\n      elif [ \"${NODE_STATUS}\" != \"FAIL\" ]; then\n        kubectl label nodes \"${node}\" preflight-status=passed --overwrite 2>/dev/null || true\n      fi\n      [ \"${FIRST_GPU}\" = \"true\" ] && FIRST_GPU=false || GPU_RESULTS=\"${GPU_RESULTS},\"\n      GPU_RESULTS=\"${GPU_RESULTS}$(cat ${dir}/${node}-gpu-health.json | tr -d '\\n')\"\n      break\n    fi\n  done\n  if [ \"${GPU_FOUND}\" != \"true\" ]; then\n    echo \"  GPU Health: MISSING\"\n    NODE_STATUS=\"FAIL\"\n    GPU_CATEGORY=\"FAIL\"\n    CLUSTER_STATUS=\"NOT_READY\"\n    FAILED_NODES=\"${FAILED_NODES}${node},\"\n    DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}gpu_health_missing:${node},\"\n    kubectl label nodes \"${node}\" preflight-status=failed --overwrite 2>/dev/null || true\n  fi\n  \n  # NCCL\n  NCCL_FOUND=false\n  for dir in \"${RESULTS_DIR}\" \"${SHARED_DIR}\"; do\n    if [ -f \"${dir}/${node}-nccl-intra.json\" ]; then\n      NCCL_FOUND=true\n      NCCL_RESULTS_FOUND=$((NCCL_RESULTS_FOUND + 1))\n      STATUS=$(grep -o '\"overall_status\"[[:space:]]*:[[:space:]]*\"[^\"]*\"' \"${dir}/${node}-nccl-intra.json\" | head -1 | cut -d'\"' -f4)\n      echo \"  NCCL: ${STATUS}\"\n      if [ \"${STATUS}\" = \"FAIL\" ]; then\n        NCCL_CATEGORY=\"FAIL\"\n        [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n        DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}nccl:${node},\"\n      fi\n      [ \"${FIRST_NCCL}\" = \"true\" ] && FIRST_NCCL=false || NCCL_RESULTS=\"${NCCL_RESULTS},\"\n      NCCL_RESULTS=\"${NCCL_RESULTS}$(cat ${dir}/${node}-nccl-intra.json | tr -d '\\n')\"\n      break\n    fi\n  done\n  if [ \"${NCCL_FOUND}\" != \"true\" ]; then\n    echo \"  NCCL: MISSING\"\n    NCCL_CATEGORY=\"FAIL\"\n    [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n    DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}nccl_missing:${node},\"\n  fi\n  \n  # Network\n  NETWORK_FOUND=false\n  for dir in \"${RESULTS_DIR}\" \"${SHARED_DIR}\"; do\n    if [ -f \"${dir}/${node}-network.json\" ]; then\n      NETWORK_FOUND=true\n      NETWORK_RESULTS_FOUND=$((NETWORK_RESULTS_FOUND + 1))\n      STATUS=$(grep -o '\"overall_status\"[[:space:]]*:[[:space:]]*\"[^\"]*\"' \"${dir}/${node}-network.json\" | head -1 | cut -d'\"' -f4)\n      echo \"  Network: ${STATUS}\"\n      if [ \"${STATUS}\" = \"FAIL\" ]; then\n        NETWORK_CATEGORY=\"FAIL\"\n        [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n        DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}network:${node},\"\n      fi\n      [ \"${FIRST_NET}\" = \"true\" ] && FIRST_NET=false || NETWORK_RESULTS=\"${NETWORK_RESULTS},\"\n      NETWORK_RESULTS=\"${NETWORK_RESULTS}$(cat ${dir}/${node}-network.json | tr -d '\\n')\"\n      break\n    fi\n  done\n  if [ \"${NETWORK_FOUND}\" != \"true\" ]; then\n    echo \"  Network: MISSING\"\n    NETWORK_CATEGORY=\"FAIL\"\n    [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n    DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}network_missing:${node},\"\n  fi\n  \n  # Storage\n  STORAGE_FOUND=false\n  for dir in \"${RESULTS_DIR}\" \"${SHARED_DIR}\"; do\n    if [ -f \"${dir}/${node}-storage.json\" ]; then\n      STORAGE_FOUND=true\n      STORAGE_RESULTS_FOUND=$((STORAGE_RESULTS_FOUND + 1))\n      STATUS=$(grep -o '\"overall_status\"[[:space:]]*:[[:space:]]*\"[^\"]*\"' \"${dir}/${node}-storage.json\" | head -1 | cut -d'\"' -f4)\n      echo \"  Storage: ${STATUS}\"\n      if [ \"${STATUS}\" = \"FAIL\" ]; then\n        STORAGE_CATEGORY=\"FAIL\"\n        [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n        DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}storage:${node},\"\n      fi\n      [ \"${FIRST_STOR}\" = \"true\" ] && FIRST_STOR=false || STORAGE_RESULTS=\"${STORAGE_RESULTS},\"\n      STORAGE_RESULTS=\"${STORAGE_RESULTS}$(cat ${dir}/${node}-storage.json | tr -d '\\n')\"\n      \n      # Extract per-node throughput for aggregate calculation\n      NODE_SEQ_READ=$(grep -o '\"bandwidth_mbps\"[[:space:]]*:[[:space:]]*[0-9.]*' \"${dir}/${node}-storage.json\" | head -1 | grep -o '[0-9.]*$' || echo 0)\n      TOTAL_SEQ_READ_MBPS=$(echo \"${TOTAL_SEQ_READ_MBPS:-0} + ${NODE_SEQ_READ}\" | bc 2>/dev/null || echo 0)\n      break\n    fi\n  done\n  if [ \"${STORAGE_FOUND}\" != \"true\" ]; then\n    echo \"  Storage: MISSING\"\n    STORAGE_CATEGORY=\"FAIL\"\n    [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n    DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}storage_missing:${node},\"\n  fi\n  \n  [ \"${NODE_STATUS}\" = \"PASS\" ] && HEALTHY_NODES=$((HEALTHY_NODES + 1))\ndone\n\nif [ \"${GPU_RESULTS_FOUND}\" -lt \"${TOTAL_GPU_NODES}\" ]; then\n  GPU_CATEGORY=\"FAIL\"\n  CLUSTER_STATUS=\"NOT_READY\"\nfi\nif [ \"${NCCL_RESULTS_FOUND}\" -lt \"${TOTAL_GPU_NODES}\" ]; then\n  NCCL_CATEGORY=\"FAIL\"\n  [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\nfi\nif [ \"${NETWORK_RESULTS_FOUND}\" -lt \"${TOTAL_GPU_NODES}\" ]; then\n  NETWORK_CATEGORY=\"FAIL\"\n  [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\nfi\nif [ \"${STORAGE_RESULTS_FOUND}\" -lt \"${TOTAL_GPU_NODES}\" ]; then\n  STORAGE_CATEGORY=\"FAIL\"\n  [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\nfi\n\n# Calculate aggregate storage throughput\nTOTAL_SEQ_READ_MBPS=${TOTAL_SEQ_READ_MBPS:-0}\necho \"\"\necho \"Aggregate Storage Throughput: ${TOTAL_SEQ_READ_MBPS} MB/s (across ${TOTAL_GPU_NODES} nodes)\"\n\nGPU_RESULTS=\"${GPU_RESULTS}]\"\nNCCL_RESULTS=\"${NCCL_RESULTS}]\"\nNETWORK_RESULTS=\"${NETWORK_RESULTS}]\"\nSTORAGE_RESULTS=\"${STORAGE_RESULTS}]\"\n\n# Scheduler results\nSCHEDULER_RESULTS=\"{}\"\nfor dir in \"${RESULTS_DIR}\" \"${SHARED_DIR}\"; do\n  if [ -f \"${dir}/scheduler-validation.json\" ]; then\n    SCHEDULER_RESULTS=$(cat \"${dir}/scheduler-validation.json\" | tr -d '\\n')\n    STATUS=$(grep -o '\"overall_status\"[[:space:]]*:[[:space:]]*\"[^\"]*\"' \"${dir}/scheduler-validation.json\" | head -1 | cut -d'\"' -f4)\n    [ \"${STATUS}\" = \"FAIL\" ] && SCHEDULER_CATEGORY=\"FAIL\"\n    break\n  fi\ndone\nif [ \"${SCHEDULER_RESULTS}\" = \"{}\" ]; then\n  SCHEDULER_CATEGORY=\"FAIL\"\n  [ \"${CLUSTER_STATUS}\" = \"READY\" ] && CLUSTER_STATUS=\"DEGRADED\"\n  DEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS}scheduler:missing,\"\nfi\n\n# Clean up trailing commas\nFAILED_NODES=\"${FAILED_NODES%,}\"\nDEGRADED_COMPONENTS=\"${DEGRADED_COMPONENTS%,}\"\n\n# Generate recommendation\ncase \"${CLUSTER_STATUS}\" in\n  \"READY\")\n    RECOMMENDATION=\"Cluster is READY for distributed GPU training. All preflight checks passed.\"\n    ;;\n  \"DEGRADED\")\n    RECOMMENDATION=\"Cluster can run training with DEGRADED performance. Review components: ${DEGRADED_COMPONENTS}. Consider remediation before production workloads.\"\n    ;;\n  \"NOT_READY\")\n    RECOMMENDATION=\"Cluster is NOT READY. Failed nodes have been tainted: ${FAILED_NODES}. Remediate hardware/software issues before training.\"\n    ;;\nesac\n\n# Generate final report\ncat > \"${FINAL_REPORT}\" << EOF\n{\n  \"cluster_preflight_report\": {\n    \"timestamp\": \"${TIMESTAMP}\",\n    \"overall_status\": \"${CLUSTER_STATUS}\",\n    \"summary\": {\n      \"total_gpu_nodes\": ${TOTAL_GPU_NODES},\n      \"healthy_nodes\": ${HEALTHY_NODES},\n      \"failed_nodes\": \"${FAILED_NODES}\",\n      \"degraded_components\": \"${DEGRADED_COMPONENTS}\",\n      \"aggregate_storage_throughput_mbps\": ${TOTAL_SEQ_READ_MBPS},\n      \"result_coverage\": {\n        \"gpu_health_reports\": ${GPU_RESULTS_FOUND},\n        \"nccl_reports\": ${NCCL_RESULTS_FOUND},\n        \"network_reports\": ${NETWORK_RESULTS_FOUND},\n        \"storage_reports\": ${STORAGE_RESULTS_FOUND}\n      }\n    },\n    \"category_status\": {\n      \"gpu_health\": \"${GPU_CATEGORY}\",\n      \"nccl_fabric\": \"${NCCL_CATEGORY}\",\n      \"network_tcp\": \"${NETWORK_CATEGORY}\",\n      \"storage\": \"${STORAGE_CATEGORY}\",\n      \"scheduler\": \"${SCHEDULER_CATEGORY}\"\n    },\n    \"recommendation\": \"${RECOMMENDATION}\",\n    \"test_results\": {\n      \"gpu_health\": ${GPU_RESULTS},\n      \"nccl_intra_node\": ${NCCL_RESULTS},\n      \"network_tcp\": ${NETWORK_RESULTS},\n      \"storage\": ${STORAGE_RESULTS},\n      \"scheduler\": ${SCHEDULER_RESULTS}\n    }\n  }\n}\nEOF\n\n# Also copy to hostPath\ncp \"${FINAL_REPORT}\" \"${RESULTS_DIR}/\" 2>/dev/null || true\n\necho \"\"\necho \"==========================================\"\necho \"       CLUSTER PREFLIGHT REPORT\"\necho \"==========================================\"\necho \"\"\necho \"CLUSTER STATUS: ${CLUSTER_STATUS}\"\necho \"\"\necho \"Summary:\"\necho \"  Total GPU Nodes: ${TOTAL_GPU_NODES}\"\necho \"  Healthy Nodes: ${HEALTHY_NODES}\"\n[ -n \"${FAILED_NODES}\" ] && echo \"  Failed Nodes: ${FAILED_NODES}\"\n[ -n \"${DEGRADED_COMPONENTS}\" ] && echo \"  Degraded Components: ${DEGRADED_COMPONENTS}\"\necho \"  Aggregate Storage Throughput: ${TOTAL_SEQ_READ_MBPS} MB/s\"\necho \"\"\necho \"Category Status:\"\necho \"  GPU Health: ${GPU_CATEGORY}\"\necho \"  NCCL Fabric: ${NCCL_CATEGORY}\"\necho \"  Network/TCP: ${NETWORK_CATEGORY}\"\necho \"  Storage: ${STORAGE_CATEGORY}\"\necho \"  Scheduler: ${SCHEDULER_CATEGORY}\"\necho \"\"\necho \"Recommendation:\"\necho \"  ${RECOMMENDATION}\"\necho \"\"\necho \"==========================================\"\n\ncase \"${CLUSTER_STATUS}\" in\n  \"READY\") echo \"CLUSTER READY FOR TRAINING\"; exit 0 ;;\n  \"DEGRADED\") echo \"CLUSTER DEGRADED - REVIEW WARNINGS\"; exit 0 ;;\n  \"NOT_READY\") echo \"CLUSTER NOT READY - REMEDIATION REQUIRED\"; exit 1 ;;\nesac\n"
          volumeMounts:
            - name: results
              mountPath: /results
            - name: shared-results
              mountPath: /shared-results
      volumes:
        - name: results
          hostPath:
            path: /var/log/gpu-preflight
            type: DirectoryOrCreate
        - name: shared-results
          hostPath:
            path: /mnt/data
            type: Directory
